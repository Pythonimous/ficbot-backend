{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:03:26.464542Z",
     "iopub.status.busy": "2025-03-13T15:03:26.464161Z",
     "iopub.status.idle": "2025-03-13T15:03:26.468912Z",
     "shell.execute_reply": "2025-03-13T15:03:26.468035Z",
     "shell.execute_reply.started": "2025-03-13T15:03:26.464511Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1000 = pd.read_csv('files/top1000_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_text(row):\n",
    "    \"\"\"\n",
    "    row: A dictionary or pandas Series containing:\n",
    "      - char_name\n",
    "      - char_bio\n",
    "      - anime_title\n",
    "      - anime_genres\n",
    "      - anime_themes\n",
    "      - anime_synopsis\n",
    "      - anime_demographic (optional)\n",
    "    \"\"\"\n",
    "    text_parts = [\n",
    "        f\"Character Name: {row['eng_name']}\",\n",
    "        f\"Character Bio: {row['bio']}\",\n",
    "        f\"Synopsis: {row['anime_synopsis']}\"\n",
    "    ]\n",
    "    text = \"\\n\".join(text_parts)\n",
    "\n",
    "    metadata = {\n",
    "        \"char_name\": row['eng_name'],\n",
    "        \"char_bio\": row['bio'],\n",
    "        \"anime_title\": row['anime_title'],\n",
    "        \"anime_synopsis\": row['anime_synopsis'],\n",
    "        \"anime_genres\": row['anime_genres'],\n",
    "        \"anime_themes\": row['anime_themes'],\n",
    "        \"anime_demographic\": \"\"\n",
    "    }\n",
    "    if 'anime_demographic' in row and row['anime_demographic']:\n",
    "        metadata[\"demographic\"] = row['anime_demographic']\n",
    "\n",
    "    return text, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a HuggingFace-based embedding model\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert into ChromaDB\n",
    "documents = []\n",
    "metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in top1000.iterrows():\n",
    "    text, metadata = create_document_text(row)\n",
    "    documents.append(text)\n",
    "    metadatas.append(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS vector store\n",
    "faiss_db = FAISS.from_texts(documents, embedding_function, metadatas=metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve closest characters\n",
    "def get_similar_characters(query_name, query_bio, k=3):\n",
    "    query_text = f\"Character Name: {query_name}\\nCharacter Bio: {query_bio}\"\n",
    "    results = faiss_db.similarity_search(query_text, k)\n",
    "\n",
    "    return results[\"metadatas\"][0]  # Extract list of metadata dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../../data/merged_characters.csv').fillna('')\n",
    "data = data[data['bio'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove short bios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from datasets import DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_length(bio):\n",
    "    return len(tokenizer(bio)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_lengths = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 78/64134 [00:00<01:23, 771.74it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1271 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 64134/64134 [00:49<00:00, 1283.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for bio in tqdm(data[\"bio\"]):\n",
    "    bio_lengths[bio] = bio_length(bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_threshold = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_short = data[data[\"bio\"].apply(lambda x: bio_lengths[x] < token_threshold)]\n",
    "data_long = data[~data[\"bio\"].apply(lambda x: bio_lengths[x] < token_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33842"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_bio_lengths = [bio_lengths[bio] for bio in data_long['bio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90th percentile: 299.0\n",
      "95th percentile: 421.0\n",
      "99th percentile: 759.5899999999965\n",
      "Max length: 4845\n"
     ]
    }
   ],
   "source": [
    "percentiles = np.percentile(long_bio_lengths, [90, 95, 99])\n",
    "\n",
    "print(f\"90th percentile: {percentiles[0]}\")\n",
    "print(f\"95th percentile: {percentiles[1]}\")\n",
    "print(f\"99th percentile: {percentiles[2]}\")\n",
    "print(f\"Max length: {max(long_bio_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 421 # roughly 95th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, tokenizer, max_length=200, stride=50):\n",
    "    \"\"\"Splits text into overlapping chunks while preserving context.\"\"\"\n",
    "    tokens = tokenizer(text)[\"input_ids\"]\n",
    "    chunks = [tokens[i : i + max_length] for i in range(0, len(tokens), max_length - stride)]\n",
    "    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_gpt2(name, bio):\n",
    "    bio_chunks = chunk_text(bio, tokenizer, max_length)\n",
    "    return [(name, chunk) for chunk in bio_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_bios = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33842/33842 [02:37<00:00, 214.91it/s]\n"
     ]
    }
   ],
   "source": [
    "for _, row in tqdm(data_long.iterrows(), total=len(data_long)):\n",
    "    name = row['eng_name']\n",
    "    bio = row['bio']\n",
    "    names_bios += format_for_gpt2(name, bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_bios = [(name.strip(), bio.strip()) for name, bio in names_bios]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "completions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36608/36608 [27:21<00:00, 22.31it/s]\n"
     ]
    }
   ],
   "source": [
    "for name, bio in tqdm(names_bios):\n",
    "    similar_characters = get_similar_characters(name, bio, k=3)\n",
    "\n",
    "    all_genres = set()\n",
    "    all_themes = set()\n",
    "\n",
    "    for char in similar_characters:\n",
    "        all_genres.update(char.metadata['anime_genres'].split('|'))\n",
    "        all_themes.update(char.metadata['anime_themes'].split('|'))\n",
    "    \n",
    "    genres = ', '.join(all_genres)\n",
    "    themes = ', '.join(all_themes)\n",
    "\n",
    "    \n",
    "    prompt = f\"[NAME] {name}\\n[GENRES] {', '.join(all_genres)}\\n[THEMES] {', '.join(all_themes)}\\n[BIO]\"\n",
    "    prompts.append(prompt)\n",
    "    completions.append(bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[NAME] Ako Udagawa\\n[GENRES] Comedy, Drama, Romance, Supernatural\\n[THEMES] Music, School, CGDCT\\n[BIO]',\n",
       " \"Year: First Year\\nBirthday: July 3rd\\nZodiac Sign: Cancer\\nLikes: Potato chips, jelly beans\\nDislikes: Eggs, green peppers\\nBand: Roselia\\nPosition: Drums\\nUdagawa Ako is a first-year student at Haneoka Girls' High School and the drummer of Roselia. She admires Minato Yukina and her elder sister Udagawa Tomoe, and is best friends with Shirokane Rinko.\")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[1000], completions[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_data = [{\"prompt\": prompt, \"completion\": completion} for prompt, completion in zip(prompts, completions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '[NAME] Houichi Kano\\n[GENRES] Supernatural, Sci-Fi, Romance, Action, Drama\\n[THEMES] Showbiz, Super Power, Gore, School, Martial Arts, Reincarnation\\n[BIO]',\n",
       " 'completion': 'The main character of the series, Houichi\\'s life is turned upside down when the Riofaldians take over the Earth by force. Houichi is called \"Gun\" by his friends, which is a joke on his name. He is called this due to the fact that the kanji symbol for Ho in his name means \"gun\" in Japanese. On the day of the tenth anniversary of first contact with the Riofaldians, Houichi meets Isaka, a beautiful, not to mention well-stacked, young high school girl who claims to be dating Houichi. She turns out to be a creation of Housuke Kano, and gives Houichi the Gunner Suit Glove, which allows Houichi to change his clothes into a power suit. After fighting off Riofaldian robots, Houichi is taken to the XXX Unit, Exaxxion.'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning_data[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"files/fine_tuning_data.jsonl\", \"w\") as f:\n",
    "    for entry in fine_tuning_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"files/name_bio_rag.jsonl\", \"r\") as f:\n",
    "    fine_tuning_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_texts = [{\"text\": example['prompt'] + f\" {example['completion']} [END]\"} for example in fine_tuning_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"files/name_bio_rag.jsonl\", \"w\") as f:\n",
    "    for entry in fine_tuning_texts:\n",
    "        f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6862448,
     "sourceId": 11020652,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 121027,
     "modelInstanceId": 100936,
     "sourceId": 120005,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ficbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
